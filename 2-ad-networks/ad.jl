### A Pluto.jl notebook ###
# v0.12.4

using Markdown
using InteractiveUtils

# This Pluto notebook uses @bind for interactivity. When running this notebook outside of Pluto, the following 'mock version' of @bind gives bound variables a default value (instead of an error).
macro bind(def, element)
    quote
        local el = $(esc(element))
        global $(esc(def)) = Core.applicable(Base.get, el) ? Base.get(el) : missing
        el
    end
end

# â•”â•â•¡ acb3cc9c-12b4-11eb-1ea6-4f4fd21b7a71
begin
	using Pkg
	Pkg.add(["PlutoUI", "Plots", "ForwardDiff"])
	using Plots
	using PlutoUI
	using ForwardDiff
end

# â•”â•â•¡ 0521b3c8-12bb-11eb-20fd-c5abc540dcd2
html"<center><button onclick=present()>Activate supermagical presentation mode</button></center>"

# â•”â•â•¡ 49ab7f72-151c-11eb-2d99-2540c575e59e
md"""
$(html"<h1><center>Crash Course in VMC</h1></center>")
$(html"<h3><center>30 minutes of Automatic Differentiation</h3></center>")

$(html"<h4><center>(in Julia)</h4></center>")

$(html"<p><center> Filippo Vicentini</center></p><p><center> filippo.vicentini@epfl.ch</center></p>")
"""

# â•”â•â•¡ 88624a8c-151e-11eb-162f-81d8f91c1cb7
md"## Setup "

# â•”â•â•¡ c5619c64-151c-11eb-3953-7da3f7df89cb
md"""
This is a fancy, super-interactive notebook on Automatic Differentiation.

No netket here... :ğŸ˜­

If you are curious... How to run this notebook:

- Install Julia: see [https://julialang.org/downloads/](https://julialang.org/downloads/)
  - MacOs: ğŸ˜Š `brew cask install julia` ğŸ˜Š 
  - Linux: ğŸ˜± Check https://julialang.org/downloads/platform/#linux_and_freebsd
    - don't use `apt intstall julia` unless you have recent distro
  - Don't tell Giuseppe

- Install dependencies (only the first time)
```
julia --project=. -e "using Pkg; Pkg.instantiate()" 
```

- Run the notebook
```
julia --project=. -e "using Pluto; Pluto.run()"
```

"""

# â•”â•â•¡ 0e96be80-1786-11eb-2cbc-b1560bcc7295
md"## Optimising a Cost function

A cost function is a _Scalar_ _real valued_ function

$\mathcal{C} : \mathbb{R}^N \rightarrow \mathbb{R}$

And to optimise it we usually need to compute it's gradient

$\vec\nabla\mathcal{C} : \mathbb{R}^N \rightarrow \mathbb{R}^N$
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathcal{W}Â \rightarrow \vec\nabla\mathcal{C}(\mathcal{W})$ 

"

# â•”â•â•¡ d7bd8802-1786-11eb-31aa-711b6ec57c69
md"""
## Gradients: the general case

Let's now consider a slightly more general case

$f : \mathbb{R}^{D_0} \rightarrow \mathbb{R}^{D_1}$


The first-order differential structure in the neighborhood of the point $x\in\mathbb{R}^{D_0}$ is encoded into the _Jacobian Matrix_

$\left(\mathcal{J}_{f}\right)^i_j(x) = \frac{(\partial f)^i}{\partial x^j}(x)$

Given another function 

$g : \mathbb{R}^{D_1} \rightarrow \mathbb{R}^{D_2}$

the Jacobian matrix for the composed function $h = g\circ f : \mathbb{R}^{D_0}\rightarrow \mathbb{R}^{D_2}$ can be written as:

$\left(\mathcal{J}_{h}\right)^i_j(x) = \frac{(\partial g)^i}{\partial y^k}(f(x))\frac{(\partial f)^k}{\partial x^j}(x)$
$\left(\mathcal{J}_{h}\right)^i_j(x) = \left(\mathcal{J}_{g}\right)^i_k(f(x)) \left(\mathcal{J}_{f}\right)^k_j(x)$

This is the chain rule
"""

# â•”â•â•¡ 1764b466-12b0-11eb-25e2-df662ae5de48
md"""# Our objective: derivatives
In the following: How to compute the derivative of a function $f(x) : \mathbb{R} \rightarrow \mathbb{R}$ on a computer.

The derivative is defined as:

$f'(x) = \frac{df}{dx}(x) = \lim_{\delta\rightarrow 0}\frac{f(x+\delta) - f(x)}{\delta}$
"""

# â•”â•â•¡ 646dade6-12b0-11eb-08c2-fb699277970f
md"""
## Roundoff errors

The easiest way to compute the derivative is by taking a small $\delta$ and using the formula above. 

Choosing a good $\delta$ is hard:

  - if $\delta$ too big $\rightarrow$ wrong result because the formula is asymptotic

  - if $\delta$ too small $\rightarrow$ wrong result because of roundoff errors:

"""


# â•”â•â•¡ 4614c9c0-12b2-11eb-324e-d10bf3b788a1
md"If I take a random (flating-point) number around $10^{-10}$...

$(@bind __Î´exp Slider(-20:20, default=-10))"

# â•”â•â•¡ fb649acc-12b1-11eb-169d-7ffdc8438666
Î´ = rand()*10.0 ^(__Î´exp)

# â•”â•â•¡ 5be24bb0-12b2-11eb-0c9b-e9d432aed109
md"The _machine epsilon_ of $\delta$ is the smallest number we can add to Î´ and obtain a number different from Î´"

# â•”â•â•¡ 0947fa56-12b2-11eb-3559-679b852517a6
eps(Î´)

# â•”â•â•¡ fc1cfad0-12b2-11eb-1312-f524f61fd631
md"Usually with double precision (8 byte) floating point number we have 16 digits of precision ($25 - 10 \approx 16$) in the *low* digits."

# â•”â•â•¡ 22693a08-12b2-11eb-0cc7-ef5ca4fca5ff
Î´â‚‚ = 1 + Î´

# â•”â•â•¡ fa39648c-12b3-11eb-0273-2d3c723d49ad
md"You see that now $Î´â‚‚$ has lost information about the lowest digits of $Î´$, because it's accurate only up to the 16-th digit."

# â•”â•â•¡ 2d4c133a-12b2-11eb-2e35-abb596e8bd84
eps(Î´â‚‚)

# â•”â•â•¡ e3404d00-151e-11eb-0401-7307e3780834
Î´Ìƒ = Î´â‚‚ - 1

# â•”â•â•¡ 62e89888-12bc-11eb-133e-911baa5701bc
md"## Finite Differencing 

So if i consider the simple function:"

# â•”â•â•¡ 73162d24-12b4-11eb-15a6-079db4539ba9
f(x) = exp(x) + sin(x);

# â•”â•â•¡ 892c68ee-12bc-11eb-36bb-5f72dec05239
md"and it's analytical derivative:"

# â•”â•â•¡ 5c7c1c24-12b5-11eb-0b5b-47016c08ceb9
dfdx(x) = exp(x) + cos(x);

# â•”â•â•¡ 84cde1a0-12b8-11eb-24a6-c72179eb1821
diff_error(diff_fun, x, Î´) = abs.(diff_fun.(f, x, Î´) .- dfdx(x));

# â•”â•â•¡ 83bd9c62-12b4-11eb-2fd9-577321bfd8eb
forward_difference(f, x, Î´) = (f(x+Î´) - f(x))/Î´; 

# â•”â•â•¡ a8a48c20-12b4-11eb-22fc-39bafed06616
# I first compute the exact values using Arbitrary precision arithmetics
Î´_arr = 10 .^ (range(-16, 0, length=100))

# â•”â•â•¡ d6e6d412-1531-11eb-19c9-e9ec766dcddc
md"I select a point xâ‚€ in it's domain and compute the derivative there: $...

$(@bind xâ‚€ Slider(-1:0.01:1, default=0.3))
"


# â•”â•â•¡ 2cb2439a-1532-11eb-2cc4-ef25533e9747
md"xâ‚€ =$xâ‚€"

# â•”â•â•¡ 6124c9f0-12b6-11eb-1cfd-2fa0a8073c57
Ïµ_fwd = diff_error(forward_difference, xâ‚€, Î´_arr)

# â•”â•â•¡ 422fcb0c-12b5-11eb-35d1-2939fcef5e31
pl = plot(Î´_arr, Ïµ_fwd, xscale=:log10, yscale=:log10, label="forward difference",  xlabel="step Î´", ylabel="error Ïµ")

# â•”â•â•¡ a2eb5c90-12bc-11eb-253c-cdb93e5d9634
md"## Finite Differencing: take 2

Instead of using the forward differencing formula, we can use more accurate central difference:"

# â•”â•â•¡ 8b6d33c2-12b5-11eb-160d-650dbb5da796
central_difference(f, x, Î´) = (f(x+Î´) - f(x-Î´))/2Î´;

# â•”â•â•¡ 97ecb6e6-12b6-11eb-1e25-25bad787aa96
Ïµ_cnt = diff_error(central_difference, xâ‚€, Î´_arr)

# â•”â•â•¡ 832609d8-12b6-11eb-06a4-17a58dd35a9b
pl2 = plot!(deepcopy(pl), Î´_arr, Ïµ_cnt, xscale=:log10, yscale=:log10, label="central difference")

# â•”â•â•¡ 32b8deea-12b7-11eb-2f14-0565a9db5882
md"And this slope also depends on the function that we are evaluating..."

# â•”â•â•¡ 8094bf72-12b7-11eb-02d6-a365f76bcb92
md"""
## Algebraic Approach: Complex step 


The problem with finite differencing is that we are mixing our really small number with the really large number, and so when we do the subtract we lose accuracy.

* We want to keep the perturbation (f'(x)) and the value (f(x)) completely separate 
"""

# â•”â•â•¡ d9100e2e-12b7-11eb-1ee2-6949a38abcd1
md"$f(x + i\delta) = f(x) + f'(x) i \delta + \mathcal{O}(\delta^2)$"

# â•”â•â•¡ 058ff8f6-12b8-11eb-2f97-d59c4fc7517a
md"$if'(x) = \frac{f(x + iÎ´) - f(x)}{\delta} + \mathcal{O}(\delta)$"

# â•”â•â•¡ 3c22b5c0-12b8-11eb-3cf0-1b5a0755450b
md"If $x$ is real and $f$ is real-valued then $if'$ is purely imaginary, therefore by taking the imaginary part of the lhs and rhs..."

# â•”â•â•¡ 2e2cd25c-12b8-11eb-0985-6de4c6fedc94
md"$f'(x) = \frac{\Im[f(x + iÎ´) + 0]}{\delta} + \mathcal{O}(\delta)$"

# â•”â•â•¡ 75e37722-12b8-11eb-2d73-691b4aa1634a
md"Let's try this approach:"

# â•”â•â•¡ 7a928a1a-12b8-11eb-2722-a96569ef6407
complex_difference(f, x, Î´) = imag(f(x+im*Î´)/Î´)

# â•”â•â•¡ c1629480-12b8-11eb-1fb9-c3a4ff1e63d2
Ïµ_cmplx = diff_error(complex_difference, xâ‚€, Î´_arr) .+ eps(Float64)

# â•”â•â•¡ d11c2ac6-12b8-11eb-30cb-8109d0a36300
pl3 = plot!(pl2, Î´_arr, Ïµ_cmplx, xscale=:log10, yscale=:log10, label="complex step")

# â•”â•â•¡ 6544e846-12b9-11eb-1ea1-8f37a7d315f3
md"WOW! practically 0 error!

This is because f is a real function of real inputs, and the step $iÎ´$ is purely imaginary, so the real and imaginary part never mix! 

No mixing $\rightarrow$ no numerical cancellation and roundoff errors!
"

# â•”â•â•¡ d8e75d86-12b9-11eb-01d0-451c26fdf2ed
md"""
### Generalizing: Sensitivities and Dual numbers

The derivative can be thought as the sensitivity of a function to it's input:

	how much does f(xâ‚€) changes when the input xâ‚€ changes by a small amount Î´?

Thanks to the Taylor's theorem:

$f(x_0+\delta) = f(x_0) + \delta f'(x_0) + ...$

And now, think about the $\delta$ as a component (a bit like the $i$ unit, quaternion directions $i,j,k$, grassman $\epsilon$...)

$\mathbb{D}(\mathbb{R}) \sim \mathbb{R}\times\mathbb{R}$

$a\in\mathbb{D}(\mathbb{R}) = a_{value} + \delta a_{sensitivity}$

$f : \mathbb{D}(\mathbb{R}) \rightarrow \mathbb{D}(\mathbb{R})$
$f(a) = f(a) + \delta f'(a)$ 

$a + b = (a_v + b_v) + \delta ( a_s + b_s)$
$a b = (a_v  b_v) + \delta ( a_s  b_v + a_v  b_s)$

"""

# â•”â•â•¡ 4830a0bc-12d0-11eb-250b-8fd51130ef12
struct Dual{T}
	val::T
	sen::T
end

# â•”â•â•¡ 49e434e6-12d0-11eb-0d5c-c1befca3ed92
Base.:+(f::Dual, g::Dual) = Dual(f.val + g.val, f.sen + g.sen)

# â•”â•â•¡ a7294eb4-12d2-11eb-0751-892ee34934c6
Base.:+(f::Dual, Î±::Number) = Dual(f.val + Î±, f.sen)

# â•”â•â•¡ a78da71a-12d2-11eb-12d1-83a0d0fc6d15
Base.:+(Î±::Number, f::Dual) = f + Î±

# â•”â•â•¡ b5bd2d70-12e5-11eb-2a70-176997854b11
Base.:*(f::Dual, g::Dual) = Dual(f.val*g.val, f.sen*g.val + f.val*g.sen)

# â•”â•â•¡ a05bf786-12e5-11eb-0aff-1358f8a0d23f
Base.exp(f::Dual) = Dual(exp(f.val), exp(f.val) * f.sen)

# â•”â•â•¡ 7967d372-1789-11eb-092c-07226f7c3a2e
Base.log(f::Dual) = Dual(exp(f.val), inv(f.val) * f.sen)

# â•”â•â•¡ 3a8b83f2-1387-11eb-1edf-75541c38c98d
Base.sin(f::Dual) = Dual(sin(f.val), cos(f.val) * f.sen)

# â•”â•â•¡ d5d8a97a-12e5-11eb-283e-e7ac4613e43c
f(Dual(xâ‚€,1.0)).sen

# â•”â•â•¡ 301e3eb6-1387-11eb-0190-2931daf82f1a
dfdx(xâ‚€)

# â•”â•â•¡ 888a9050-1532-11eb-1823-c327dd8d6834
md"""
However, think about the function
"""

# â•”â•â•¡ f7a98b50-1788-11eb-30d0-a163d703e1fd
md"""
##Â An example

A nested function 

"""

# â•”â•â•¡ 0ccf3c6e-1789-11eb-03ff-dd75fff32410
h = exp âˆ˜ sin âˆ˜ log

# â•”â•â•¡ 206bbb3a-1789-11eb-3c83-15d41dcd4d23
plot(range(0.01, 3.0, length=100), h)

# â•”â•â•¡ 51bbba96-1789-11eb-218e-ebc94527410a
#Â Forward mode:
xáµ¢ = Dual(0.5, 1.0)

# â•”â•â•¡ 62dcd53a-1789-11eb-13f3-39d907a8ba76
yâ‚ = log(xáµ¢)

# â•”â•â•¡ 9e845f9a-1789-11eb-1760-59dfdf42d6e5
yâ‚‚ = sin(yâ‚)

# â•”â•â•¡ b9982c44-1789-11eb-3c11-31655a8c77cf
yâ‚ƒ = sin(yâ‚‚)

# â•”â•â•¡ Cell order:
# â•Ÿâ”€0521b3c8-12bb-11eb-20fd-c5abc540dcd2
# â•Ÿâ”€49ab7f72-151c-11eb-2d99-2540c575e59e
# â•Ÿâ”€88624a8c-151e-11eb-162f-81d8f91c1cb7
# â•Ÿâ”€c5619c64-151c-11eb-3953-7da3f7df89cb
# â• â•acb3cc9c-12b4-11eb-1ea6-4f4fd21b7a71
# â• â•0e96be80-1786-11eb-2cbc-b1560bcc7295
# â•Ÿâ”€d7bd8802-1786-11eb-31aa-711b6ec57c69
# â• â•1764b466-12b0-11eb-25e2-df662ae5de48
# â•Ÿâ”€646dade6-12b0-11eb-08c2-fb699277970f
# â•Ÿâ”€4614c9c0-12b2-11eb-324e-d10bf3b788a1
# â•Ÿâ”€fb649acc-12b1-11eb-169d-7ffdc8438666
# â•Ÿâ”€5be24bb0-12b2-11eb-0c9b-e9d432aed109
# â• â•0947fa56-12b2-11eb-3559-679b852517a6
# â•Ÿâ”€fc1cfad0-12b2-11eb-1312-f524f61fd631
# â• â•22693a08-12b2-11eb-0cc7-ef5ca4fca5ff
# â•Ÿâ”€fa39648c-12b3-11eb-0273-2d3c723d49ad
# â• â•2d4c133a-12b2-11eb-2e35-abb596e8bd84
# â• â•e3404d00-151e-11eb-0401-7307e3780834
# â•Ÿâ”€84cde1a0-12b8-11eb-24a6-c72179eb1821
# â•Ÿâ”€62e89888-12bc-11eb-133e-911baa5701bc
# â• â•73162d24-12b4-11eb-15a6-079db4539ba9
# â•Ÿâ”€892c68ee-12bc-11eb-36bb-5f72dec05239
# â• â•5c7c1c24-12b5-11eb-0b5b-47016c08ceb9
# â• â•83bd9c62-12b4-11eb-2fd9-577321bfd8eb
# â• â•a8a48c20-12b4-11eb-22fc-39bafed06616
# â•Ÿâ”€d6e6d412-1531-11eb-19c9-e9ec766dcddc
# â•Ÿâ”€2cb2439a-1532-11eb-2cc4-ef25533e9747
# â• â•6124c9f0-12b6-11eb-1cfd-2fa0a8073c57
# â• â•422fcb0c-12b5-11eb-35d1-2939fcef5e31
# â•Ÿâ”€a2eb5c90-12bc-11eb-253c-cdb93e5d9634
# â• â•8b6d33c2-12b5-11eb-160d-650dbb5da796
# â• â•97ecb6e6-12b6-11eb-1e25-25bad787aa96
# â• â•832609d8-12b6-11eb-06a4-17a58dd35a9b
# â•Ÿâ”€32b8deea-12b7-11eb-2f14-0565a9db5882
# â•Ÿâ”€8094bf72-12b7-11eb-02d6-a365f76bcb92
# â•Ÿâ”€d9100e2e-12b7-11eb-1ee2-6949a38abcd1
# â•Ÿâ”€058ff8f6-12b8-11eb-2f97-d59c4fc7517a
# â•Ÿâ”€3c22b5c0-12b8-11eb-3cf0-1b5a0755450b
# â•Ÿâ”€2e2cd25c-12b8-11eb-0985-6de4c6fedc94
# â•Ÿâ”€75e37722-12b8-11eb-2d73-691b4aa1634a
# â• â•7a928a1a-12b8-11eb-2722-a96569ef6407
# â• â•c1629480-12b8-11eb-1fb9-c3a4ff1e63d2
# â• â•d11c2ac6-12b8-11eb-30cb-8109d0a36300
# â•Ÿâ”€6544e846-12b9-11eb-1ea1-8f37a7d315f3
# â•Ÿâ”€d8e75d86-12b9-11eb-01d0-451c26fdf2ed
# â• â•4830a0bc-12d0-11eb-250b-8fd51130ef12
# â• â•49e434e6-12d0-11eb-0d5c-c1befca3ed92
# â• â•a7294eb4-12d2-11eb-0751-892ee34934c6
# â• â•a78da71a-12d2-11eb-12d1-83a0d0fc6d15
# â• â•b5bd2d70-12e5-11eb-2a70-176997854b11
# â• â•a05bf786-12e5-11eb-0aff-1358f8a0d23f
# â• â•7967d372-1789-11eb-092c-07226f7c3a2e
# â• â•3a8b83f2-1387-11eb-1edf-75541c38c98d
# â• â•d5d8a97a-12e5-11eb-283e-e7ac4613e43c
# â• â•301e3eb6-1387-11eb-0190-2931daf82f1a
# â• â•888a9050-1532-11eb-1823-c327dd8d6834
# â•Ÿâ”€f7a98b50-1788-11eb-30d0-a163d703e1fd
# â• â•0ccf3c6e-1789-11eb-03ff-dd75fff32410
# â• â•206bbb3a-1789-11eb-3c83-15d41dcd4d23
# â• â•51bbba96-1789-11eb-218e-ebc94527410a
# â• â•62dcd53a-1789-11eb-13f3-39d907a8ba76
# â• â•9e845f9a-1789-11eb-1760-59dfdf42d6e5
# â• â•b9982c44-1789-11eb-3c11-31655a8c77cf
